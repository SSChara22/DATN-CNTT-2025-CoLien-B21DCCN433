{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6540d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: 42, Items: 139, Train rows: 3373, Test positives: 217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 91.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: avg_loss=0.714706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 82.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: avg_loss=0.709957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 62.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: avg_loss=0.706248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 77.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: avg_loss=0.702621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 73.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: avg_loss=0.698361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 83.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: avg_loss=0.693128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 83.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: avg_loss=0.686644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 85.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: avg_loss=0.678146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 112.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: avg_loss=0.666274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 83.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: avg_loss=0.650796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval LOO: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 217/217 [00:00<00:00, 3189.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "Precision@10: 0.3548\n",
      "MAP@10:       0.1263\n",
      "===================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# NeuMF training + leave-one-out evaluation (sample 99 negatives per positive)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "\n",
    "# -------------------------\n",
    "# Paths\n",
    "# -------------------------\n",
    "train_path = './output_neumf/neumf_train.csv'\n",
    "test_path  = './output_neumf/neumf_test.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df  = pd.read_csv(test_path)\n",
    "\n",
    "# Keep only necessary cols and ensure types\n",
    "train_df = train_df[['user','item','label']].copy()\n",
    "test_df  = test_df[['user','item','label']].copy()\n",
    "\n",
    "# Ensure positive-only test for holdout evaluation\n",
    "test_pos_df = test_df[test_df['label'] == 1].copy()\n",
    "if test_pos_df.empty:\n",
    "    raise RuntimeError(\"Kh√¥ng t√¨m th·∫•y sample positive trong test_df (label==1). H√£y ƒë·∫£m b·∫£o test ch·ª©a positives holdout.\")\n",
    "\n",
    "# -------------------------\n",
    "# Encode users/items\n",
    "# -------------------------\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "\n",
    "# Fit encoders on union of train+test to avoid unseen labels at inference\n",
    "all_users = pd.concat([train_df['user'], test_pos_df['user']]).astype(str)\n",
    "all_items = pd.concat([train_df['item'], test_pos_df['item']]).astype(str)\n",
    "user_encoder.fit(all_users)\n",
    "item_encoder.fit(all_items)\n",
    "\n",
    "train_df['user'] = user_encoder.transform(train_df['user'].astype(str))\n",
    "train_df['item'] = item_encoder.transform(train_df['item'].astype(str))\n",
    "test_pos_df['user'] = user_encoder.transform(test_pos_df['user'].astype(str))\n",
    "test_pos_df['item'] = item_encoder.transform(test_pos_df['item'].astype(str))\n",
    "\n",
    "n_users = len(user_encoder.classes_)\n",
    "n_items = len(item_encoder.classes_)\n",
    "\n",
    "print(f\"Users: {n_users}, Items: {n_items}, Train rows: {len(train_df)}, Test positives: {len(test_pos_df)}\")\n",
    "\n",
    "# -------------------------\n",
    "# Build train interaction set (to avoid sampling seen items as negatives)\n",
    "# -------------------------\n",
    "train_user_pos = train_df[train_df['label']==1].groupby('user')['item'].apply(set).to_dict()\n",
    "# ensure users with no positives have empty set\n",
    "for u in range(n_users):\n",
    "    if u not in train_user_pos:\n",
    "        train_user_pos[u] = set()\n",
    "\n",
    "# -------------------------\n",
    "# Dataset & DataLoader (pointwise)\n",
    "# -------------------------\n",
    "class RatingDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.users = torch.tensor(df['user'].values, dtype=torch.long)\n",
    "        self.items = torch.tensor(df['item'].values, dtype=torch.long)\n",
    "        self.labels = torch.tensor(df['label'].values, dtype=torch.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.labels[idx]\n",
    "\n",
    "train_loader = DataLoader(RatingDataset(train_df), batch_size=1024, shuffle=True, num_workers=0)\n",
    "\n",
    "# -------------------------\n",
    "# NeuMF model (slightly larger)\n",
    "# -------------------------\n",
    "class NeuMF(nn.Module):\n",
    "    def __init__(self, n_users, n_items, mf_dim=32, mlp_layers=[128,64,32,16], dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.user_emb_gmf = nn.Embedding(n_users, mf_dim)\n",
    "        self.item_emb_gmf = nn.Embedding(n_items, mf_dim)\n",
    "        self.user_emb_mlp = nn.Embedding(n_users, mlp_layers[0]//2)\n",
    "        self.item_emb_mlp = nn.Embedding(n_items, mlp_layers[0]//2)\n",
    "        mlp = []\n",
    "        input_dim = mlp_layers[0]\n",
    "        for dim in mlp_layers[1:]:\n",
    "            mlp.append(nn.Linear(input_dim, dim))\n",
    "            mlp.append(nn.ReLU())\n",
    "            if dropout>0:\n",
    "                mlp.append(nn.Dropout(dropout))\n",
    "            input_dim = dim\n",
    "        self.mlp = nn.Sequential(*mlp)\n",
    "        self.fc = nn.Linear(mf_dim + mlp_layers[-1], 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # init\n",
    "        nn.init.normal_(self.user_emb_gmf.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_emb_gmf.weight, std=0.01)\n",
    "        nn.init.normal_(self.user_emb_mlp.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_emb_mlp.weight, std=0.01)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        gmf = self.user_emb_gmf(user) * self.item_emb_gmf(item)\n",
    "        mlp_in = torch.cat([self.user_emb_mlp(user), self.item_emb_mlp(item)], dim=-1)\n",
    "        mlp_out = self.mlp(mlp_in)\n",
    "        concat = torch.cat([gmf, mlp_out], dim=-1)\n",
    "        out = self.fc(concat)\n",
    "        return self.sigmoid(out).squeeze()\n",
    "\n",
    "# -------------------------\n",
    "# Training setup\n",
    "# -------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = NeuMF(n_users, n_items, mf_dim=32, mlp_layers=[128,64,32,16], dropout=0.0).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# -------------------------\n",
    "# Train\n",
    "# -------------------------\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for users, items, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        users = users.to(device); items = items.to(device); labels = labels.to(device)\n",
    "        preds = model(users, items)\n",
    "        loss = criterion(preds, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * users.size(0)\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: avg_loss={avg_loss:.6f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Evaluation: leave-one-out with sampled negatives\n",
    "# For each positive (user, pos_item) in test_pos_df:\n",
    "#   sample N_NEG negatives from items NOT in user's train positives AND NOT equal pos_item\n",
    "#   rank pos_item among negatives by predicted score\n",
    "# -------------------------\n",
    "def precision_at_k_single(pos_index, ranked_list, k=10):\n",
    "    return 1.0 if pos_index < k else 0.0\n",
    "\n",
    "def average_precision_single(pos_index, ranked_list, k=10):\n",
    "    # In leave-one-out with single positive, AP is 1/(rank) if rank<=k else 0\n",
    "    if pos_index < k:\n",
    "        return 1.0 / (pos_index + 1.0)\n",
    "    return 0.0\n",
    "\n",
    "def evaluate_leave_one_out(model, test_pos_df, train_user_pos, n_items, n_negatives=99, k=10, seed=42):\n",
    "    rng = random.Random(seed)\n",
    "    model.eval()\n",
    "    precisions = []\n",
    "    aps = []\n",
    "    users = test_pos_df['user'].values\n",
    "    items = test_pos_df['item'].values\n",
    "    with torch.no_grad():\n",
    "        for u, pos_item in tqdm(zip(users, items), total=len(users), desc=\"Eval LOO\"):\n",
    "            # sample negatives\n",
    "            neg_pool = set(range(n_items)) - train_user_pos.get(u, set()) - {pos_item}\n",
    "            # if pool smaller than required, take all\n",
    "            neg_list = rng.sample(list(neg_pool), k=min(n_negatives, len(neg_pool)))\n",
    "            candidate_items = neg_list + [pos_item]\n",
    "            # compute scores\n",
    "            user_tensor = torch.tensor([u]*len(candidate_items), dtype=torch.long).to(device)\n",
    "            item_tensor = torch.tensor(candidate_items, dtype=torch.long).to(device)\n",
    "            scores = model(user_tensor, item_tensor).cpu().numpy()\n",
    "            # rank descending\n",
    "            ranked_idx = np.argsort(-scores)\n",
    "            # find rank (0-based) of pos_item\n",
    "            pos_rank = int(np.where(np.array(candidate_items)[ranked_idx] == pos_item)[0][0])\n",
    "            precisions.append(precision_at_k_single(pos_rank, ranked_idx, k))\n",
    "            aps.append(average_precision_single(pos_rank, ranked_idx, k))\n",
    "    return np.mean(precisions), np.mean(aps)\n",
    "\n",
    "precision10, map10 = evaluate_leave_one_out(model, test_pos_df, train_user_pos, n_items, n_negatives=99, k=10)\n",
    "print(\"===================================================\")\n",
    "print(f\"Precision@10: {precision10:.4f}\")\n",
    "print(f\"MAP@10:       {map10:.4f}\")\n",
    "print(\"===================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8eca209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: 42, Items: 139, Contexts: 27\n",
      "Using device: cpu\n",
      "\n",
      "=== Training LNCM_Fixed ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LNCM] Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 390.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.6332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LNCM] Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 357.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 0.5569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LNCM] Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 416.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 0.5260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LNCM] Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 367.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 0.4934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LNCM] Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 402.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 0.4596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LNCM] Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 382.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 0.4543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LNCM] Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 382.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 0.4434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LNCM] Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 431.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 0.4252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LNCM] Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 341.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 0.4247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LNCM] Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 350.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.4158\n",
      "\n",
      "=== Evaluating  ===\n",
      "\n",
      "--- Basic Metrics ---\n",
      "AUC: 0.7841\n",
      "RMSE: 0.3911\n",
      "MAE: 0.2851\n",
      "\n",
      "--- Ranking Metrics @ 10 ---\n",
      "Precision@10: 0.3414\n",
      "Recall@10:    0.7727\n",
      "Hit@10:       1.0000\n",
      "MAP@10:       0.5073\n",
      "\n",
      "‚úÖ LNCM Training & Evaluation Complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# LNCM (Latent Neural Context Model)\n",
    "# =====================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# =====================================================\n",
    "# 1Ô∏è‚É£ LOAD & CHU·∫®N B·ªä D·ªÆ LI·ªÜU\n",
    "# =====================================================\n",
    "# ƒê·ªçc file d·ªØ li·ªáu g·ªìm user, item, label (rating) v√† context_id\n",
    "df = pd.read_csv('output_carskit_clean/ratings_with_context_id.csv')\n",
    "\n",
    "# üî∏ M·ª•c ƒë√≠ch: √°nh x·∫° user/item/context sang ID li√™n t·ª•c (0,1,2,...)\n",
    "#  -> gi√∫p embedding layer c√≥ th·ªÉ x·ª≠ l√Ω ƒë∆∞·ª£c\n",
    "user2id = {u: i for i, u in enumerate(df['user'].unique())}\n",
    "item2id = {i: j for j, i in enumerate(df['item'].unique())}\n",
    "context2id = {c: k for k, c in enumerate(df['context_id'].unique())}\n",
    "\n",
    "# Thay th·∫ø c√°c gi√° tr·ªã g·ªëc b·∫±ng ID\n",
    "df['user'] = df['user'].map(user2id)\n",
    "df['item'] = df['item'].map(item2id)\n",
    "df['context'] = df['context_id'].map(context2id)\n",
    "\n",
    "# Th·ªëng k√™ s·ªë l∆∞·ª£ng ph·∫ßn t·ª≠\n",
    "n_users = len(user2id)\n",
    "n_items = len(item2id)\n",
    "n_contexts = len(context2id)\n",
    "print(f\"Users: {n_users}, Items: {n_items}, Contexts: {n_contexts}\")\n",
    "\n",
    "# üîπ Chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm th·ª≠\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# =====================================================\n",
    "# 1.1Ô∏è‚É£ ƒê·ªäNH NGHƒ®A Dataset CHO PYTORCH\n",
    "# =====================================================\n",
    "class RatingDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset c∆° b·∫£n cho b√†i to√°n g·ª£i √Ω c√≥ ng·ªØ c·∫£nh\"\"\"\n",
    "    def __init__(self, df):\n",
    "        # √âp ki·ªÉu v·ªÅ tensor ƒë·ªÉ ti·ªán cho GPU\n",
    "        self.users = torch.tensor(df['user'].values, dtype=torch.long)\n",
    "        self.items = torch.tensor(df['item'].values, dtype=torch.long)\n",
    "        self.contexts = torch.tensor(df['context'].values, dtype=torch.long)\n",
    "        self.labels = torch.tensor(df['label'].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.contexts[idx], self.labels[idx]\n",
    "\n",
    "# T·∫°o DataLoader ƒë·ªÉ hu·∫•n luy·ªán theo batch\n",
    "train_data = RatingDataset(train_df)\n",
    "test_data = RatingDataset(test_df)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=256, shuffle=False)\n",
    "\n",
    "# =====================================================\n",
    "# 2Ô∏è‚É£ M√î H√åNH: LNCM (Latent Neural Context Model)\n",
    "# =====================================================\n",
    "class LNCM_Fixed(nn.Module):\n",
    "    \"\"\"\n",
    "    üîπ √ù t∆∞·ªüng:\n",
    "        - LNCM kh√¥ng d√πng embedding context tr·ª±c ti·∫øp.\n",
    "        - Thay v√†o ƒë√≥, m√¥ h√¨nh \"t·ª± sinh\" vector ng·ªØ c·∫£nh ti·ªÅm ·∫©n (latent context)\n",
    "          t·ª´ c·∫∑p (user, item) th√¥ng qua m·ªôt encoder ki·ªÉu VAE (Variational Autoencoder).\n",
    "    üîπ M·ª•c ti√™u:\n",
    "        - H·ªçc ƒë∆∞·ª£c t√°c ƒë·ªông ng·∫ßm c·ªßa ng·ªØ c·∫£nh ngay c·∫£ khi context kh√¥ng r√µ r√†ng.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_users, n_items, n_contexts, emb_dim=32, latent_dim=16):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1Ô∏è‚É£ Embedding cho user v√† item\n",
    "        self.user_emb = nn.Embedding(n_users, emb_dim)\n",
    "        self.item_emb = nn.Embedding(n_items, emb_dim)\n",
    "\n",
    "        # 2Ô∏è‚É£ Encoder sinh ng·ªØ c·∫£nh ti·ªÅm ·∫©n (latent context)\n",
    "        #    Input: [user_emb, item_emb]\n",
    "        #    Output: [mu, logvar] d√πng cho VAE reparameterization trick\n",
    "        self.context_encoder = nn.Sequential(\n",
    "            nn.Linear(emb_dim * 2, latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_dim, emb_dim * 2)  # output g·ªìm 2 ph·∫ßn: mu & logvar\n",
    "        )\n",
    "\n",
    "        # 3Ô∏è‚É£ M·∫°ng fully-connected d·ª± ƒëo√°n x√°c su·∫•t user th√≠ch item\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(emb_dim * 3, 64),  # user + item + latent context\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # 4Ô∏è‚É£ Kh·ªüi t·∫°o tr·ªçng s·ªë embedding\n",
    "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
    "\n",
    "    # === H√†m t√°i tham s·ªë h√≥a trong VAE ===\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Chuy·ªÉn (mu, logvar) ‚Üí latent vector theo c√¥ng th·ª©c: z = mu + eps * sigma\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, user, item, context=None):\n",
    "        \"\"\"Lan truy·ªÅn ti·∫øn (forward pass)\"\"\"\n",
    "        # Embedding user v√† item\n",
    "        u = self.user_emb(user)\n",
    "        i = self.item_emb(item)\n",
    "\n",
    "        # Encoder sinh (mu, logvar) ‚Üí latent context\n",
    "        context_params = self.context_encoder(torch.cat([u, i], dim=1))\n",
    "        mu, logvar = torch.chunk(context_params, 2, dim=1)\n",
    "        c_latent = self.reparameterize(mu, logvar)\n",
    "\n",
    "        # Gh√©p t·∫•t c·∫£ embedding l·∫°i ƒë·ªÉ d·ª± ƒëo√°n\n",
    "        x = torch.cat([u, i, c_latent], dim=1)\n",
    "        return self.fc(x).squeeze()\n",
    "\n",
    "# =====================================================\n",
    "# 3Ô∏è‚É£ V√íNG HU·∫§N LUY·ªÜN M√î H√åNH\n",
    "# =====================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_lncm = LNCM_Fixed(n_users, n_items, n_contexts).to(device)\n",
    "optimizer = optim.Adam(model_lncm.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCELoss()  # Binary Cross-Entropy cho nh√£n 0/1\n",
    "\n",
    "print(\"\\n=== Training LNCM_Fixed ===\")\n",
    "for epoch in range(10):\n",
    "    model_lncm.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Duy·ªát t·ª´ng batch d·ªØ li·ªáu\n",
    "    for users, items, contexts, labels in tqdm(train_loader, desc=f\"[LNCM] Epoch {epoch+1}\"):\n",
    "        users, items, labels = users.to(device), items.to(device), labels.to(device)\n",
    "\n",
    "        preds = model_lncm(users, items)      # D·ª± ƒëo√°n\n",
    "        loss = loss_fn(preds, labels)         # T√≠nh loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()                       # Lan truy·ªÅn ng∆∞·ª£c\n",
    "        optimizer.step()                       # C·∫≠p nh·∫≠t tr·ªçng s·ªë\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# =====================================================\n",
    "# 4Ô∏è‚É£ ƒê√ÅNH GI√Å M√î H√åNH\n",
    "# =====================================================\n",
    "def evaluate_ranking_metrics(model, test_loader, device, K=10):\n",
    "    \"\"\"\n",
    "    ƒê√°nh gi√° m√¥ h√¨nh theo:\n",
    "    - Basic metrics: AUC, RMSE, MAE\n",
    "    - Ranking metrics: Precision@10, Recall@10, Hit@10, MAP@10\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    user_preds = {}\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for users, items, contexts, labels in test_loader:\n",
    "            users, items, contexts = users.to(device), items.to(device), contexts.to(device)\n",
    "            preds = model(users, items, contexts).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "            for u, i, p, l in zip(users.cpu().numpy(), items.cpu().numpy(), preds, labels.numpy()):\n",
    "                if u not in user_preds:\n",
    "                    user_preds[u] = []\n",
    "                user_preds[u].append((i, p, l))\n",
    "\n",
    "    # === Basic metrics ===\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "    rmse = np.sqrt(np.mean((all_labels - all_preds) ** 2))\n",
    "    mae = np.mean(np.abs(all_labels - all_preds))\n",
    "\n",
    "    print(f\"\\n--- Basic Metrics ---\")\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "\n",
    "    # === Ranking metrics @K ===\n",
    "    precisions, recalls, hits, maps = [], [], [], []\n",
    "\n",
    "    for user, predictions in user_preds.items():\n",
    "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "        total_relevant = sum([l for _, _, l in predictions])\n",
    "        if total_relevant == 0:\n",
    "            continue\n",
    "\n",
    "        top_k = predictions[:K]\n",
    "        top_k_labels = [l for _, _, l in top_k]\n",
    "\n",
    "        # Precision, Recall, Hit\n",
    "        precision = sum(top_k_labels) / K\n",
    "        recall = sum(top_k_labels) / total_relevant\n",
    "        hit = 1.0 if sum(top_k_labels) > 0 else 0.0\n",
    "\n",
    "        # MAP@K\n",
    "        ap_sum, correct = 0.0, 0\n",
    "        for idx, (_, _, label) in enumerate(top_k, start=1):\n",
    "            if label == 1:\n",
    "                correct += 1\n",
    "                ap_sum += correct / idx\n",
    "        map_k = ap_sum / min(total_relevant, K)\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        hits.append(hit)\n",
    "        maps.append(map_k)\n",
    "\n",
    "    # In k·∫øt qu·∫£ trung b√¨nh\n",
    "    print(f\"\\n--- Ranking Metrics @ {K} ---\")\n",
    "    print(f\"Precision@{K}: {np.mean(precisions):.4f}\")\n",
    "    print(f\"Recall@{K}:    {np.mean(recalls):.4f}\")\n",
    "    print(f\"Hit@{K}:       {np.mean(hits):.4f}\")\n",
    "    print(f\"MAP@{K}:       {np.mean(maps):.4f}\")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 5Ô∏è‚É£ CH·∫†Y ƒê√ÅNH GI√Å SAU HU·∫§N LUY·ªÜN\n",
    "# =====================================================\n",
    "print(\"\\n=== Evaluating  ===\")\n",
    "evaluate_ranking_metrics(model_lncm, test_loader, device)\n",
    "\n",
    "print(\"\\n‚úÖ LNCM Training & Evaluation Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c400de85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: 42, Items: 139, Contexts: 27\n",
      "Using device: cpu\n",
      "\n",
      "=== Training ENCM ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 307.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.6332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 312.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 0.5592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 346.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 0.5106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 253.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 0.4671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 306.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 0.4573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 309.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 0.4310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 305.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 0.4215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 318.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 0.4126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 328.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 0.3998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 307.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.3941\n",
      "\n",
      "=== Evaluating ENCM ===\n",
      "\n",
      "=== Evaluating ENCM ===\n",
      "\n",
      "--- Basic Metrics ---\n",
      "AUC: 0.7968\n",
      "RMSE: 0.3882\n",
      "MAE: 0.2812\n",
      "\n",
      "--- Ranking Metrics @ 10 ---\n",
      "Precision@10: 0.3448\n",
      "Recall@10:    0.7876\n",
      "Hit@10:       1.0000\n",
      "MAP@10:       0.5520\n",
      "\n",
      "‚úÖ ENCM Training & Evaluation Complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# üìö Explicit Neural Context Model (ENCM)\n",
    "# =====================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# T·∫¢I V√Ä X·ª¨ L√ù D·ªÆ LI·ªÜU\n",
    "# -----------------------------------------------------\n",
    "# D·ªØ li·ªáu g·ªìm 4 c·ªôt ch√≠nh: user, item, label (0/1), context_id\n",
    "# context_id th·ªÉ hi·ªán c√°c ƒëi·ªÅu ki·ªán nh∆∞ th·ªùi ti·∫øt, t√¢m tr·∫°ng, ƒë∆∞·ªùng, v.v.\n",
    "df = pd.read_csv('output_carskit_clean/ratings_with_context_id.csv')\n",
    "\n",
    "# √Ånh x·∫° id r·ªùi r·∫°c th√†nh ch·ªâ s·ªë li√™n t·ª•c ƒë·ªÉ d√πng cho embedding\n",
    "user2id = {u: i for i, u in enumerate(df['user'].unique())}\n",
    "item2id = {i: j for j, i in enumerate(df['item'].unique())}\n",
    "context2id = {c: k for k, c in enumerate(df['context_id'].unique())}\n",
    "\n",
    "# G√°n l·∫°i id li√™n t·ª•c\n",
    "df['user'] = df['user'].map(user2id)\n",
    "df['item'] = df['item'].map(item2id)\n",
    "df['context'] = df['context_id'].map(context2id)\n",
    "\n",
    "# Th·ªëng k√™ s·ªë l∆∞·ª£ng th·ª±c th·ªÉ\n",
    "n_users = len(user2id)\n",
    "n_items = len(item2id)\n",
    "n_contexts = len(context2id)\n",
    "print(f\"Users: {n_users}, Items: {n_items}, Contexts: {n_contexts}\")\n",
    "\n",
    "# Chia train/test (80% train, 20% test)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# T·∫†O DATASET CHO PYTORCH\n",
    "# -----------------------------------------------------\n",
    "class RatingDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset chu·∫©n cho b√†i to√°n recommendation.\"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.users = torch.tensor(df['user'].values, dtype=torch.long)\n",
    "        self.items = torch.tensor(df['item'].values, dtype=torch.long)\n",
    "        self.contexts = torch.tensor(df['context'].values, dtype=torch.long)\n",
    "        self.labels = torch.tensor(df['label'].values, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.contexts[idx], self.labels[idx]\n",
    "\n",
    "# DataLoader ƒë·ªÉ chia batch\n",
    "train_data = RatingDataset(train_df)\n",
    "test_data = RatingDataset(test_df)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=256, shuffle=False)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# ƒê·ªäNH NGHƒ®A M√î H√åNH ENCM\n",
    "# -----------------------------------------------------\n",
    "class ENCM(nn.Module):\n",
    "    \"\"\"\n",
    "    ENCM - Explicit Neural Context Model\n",
    "    ------------------------------------\n",
    "    √ù t∆∞·ªüng:\n",
    "      - M·ªói user, item, context ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·∫±ng embedding.\n",
    "      - Context kh√¥ng ch·ªâ ƒë∆∞·ª£c n·ªëi v√†o m√† c√≤n *ƒëi·ªÅu ch·ªânh (modulate)*\n",
    "        embedding c·ªßa user v√† item th√¥ng qua *gating mechanism*.\n",
    "      - M·ª•c ti√™u: h·ªçc ·∫£nh h∆∞·ªüng c·ªßa context ƒë·∫øn h√†nh vi user r√µ r√†ng h∆°n.\n",
    "\n",
    "    Ki·∫øn tr√∫c:\n",
    "      user/item embedding ‚Üí context gate ‚Üí modulation ‚Üí concat ‚Üí MLP ‚Üí output\n",
    "    \"\"\"\n",
    "    def __init__(self, n_users, n_items, n_contexts, emb_dim=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- 3 lo·∫°i embedding c∆° b·∫£n ---\n",
    "        self.user_emb = nn.Embedding(n_users, emb_dim)\n",
    "        self.item_emb = nn.Embedding(n_items, emb_dim)\n",
    "        self.context_emb = nn.Embedding(n_contexts, emb_dim)\n",
    "        \n",
    "        # --- Gating: context ·∫£nh h∆∞·ªüng l√™n user/item ---\n",
    "        # Gate l√† 1 h√†m sigmoid ƒë·ªÉ t·∫°o tr·ªçng s·ªë ƒëi·ªÅu ch·ªânh ƒë·ªông cho t·ª´ng chi·ªÅu embedding\n",
    "        self.user_gate = nn.Sequential(\n",
    "            nn.Linear(emb_dim * 2, emb_dim),  # [user, context] -> gate vector\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.item_gate = nn.Sequential(\n",
    "            nn.Linear(emb_dim * 2, emb_dim),  # [item, context] -> gate vector\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # --- M·∫°ng d·ª± ƒëo√°n (prediction network) ---\n",
    "        # Nh·∫≠n ƒë·∫ßu v√†o l√† [user_modulated, item_modulated, context]\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(emb_dim * 3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()  # ƒë·∫ßu ra x√°c su·∫•t\n",
    "        )\n",
    "        \n",
    "        # --- Kh·ªüi t·∫°o tr·ªçng s·ªë embedding ---\n",
    "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.context_emb.weight)\n",
    "    \n",
    "    def forward(self, user, item, context):\n",
    "        # 1Ô∏è‚É£ L·∫•y embedding c∆° b·∫£n\n",
    "        u = self.user_emb(user)\n",
    "        i = self.item_emb(item)\n",
    "        c = self.context_emb(context)\n",
    "        \n",
    "        # 2Ô∏è‚É£ Context ƒëi·ªÅu ch·ªânh user\n",
    "        u_context = torch.cat([u, c], dim=1)\n",
    "        u_gate = self.user_gate(u_context)      # gate ‚àà [0,1]^emb_dim\n",
    "        u_modulated = u * u_gate                # nh√¢n t·ª´ng chi·ªÅu ƒë·ªÉ ƒëi·ªÅu ch·ªânh\n",
    "        \n",
    "        # 3Ô∏è‚É£ Context ƒëi·ªÅu ch·ªânh item\n",
    "        i_context = torch.cat([i, c], dim=1)\n",
    "        i_gate = self.item_gate(i_context)\n",
    "        i_modulated = i * i_gate\n",
    "        \n",
    "        # 4Ô∏è‚É£ K·∫øt h·ª£p t·∫•t c·∫£ embeddings l·∫°i\n",
    "        x = torch.cat([u_modulated, i_modulated, c], dim=1)\n",
    "        \n",
    "        # 5Ô∏è‚É£ D·ª± ƒëo√°n x√°c su·∫•t user th√≠ch item trong context\n",
    "        return self.fc(x).squeeze()\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# HU·∫§N LUY·ªÜN M√î H√åNH\n",
    "# -----------------------------------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = ENCM(n_users, n_items, n_contexts, emb_dim=32).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "loss_fn = nn.BCELoss()  # V√¨ b√†i to√°n nh·ªã ph√¢n (0/1)\n",
    "\n",
    "print(\"\\n=== Training ENCM ===\")\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Duy·ªát t·ª´ng batch trong d·ªØ li·ªáu hu·∫•n luy·ªán\n",
    "    for users, items, contexts, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        users, items, contexts, labels = users.to(device), items.to(device), contexts.to(device), labels.to(device)\n",
    "        \n",
    "        preds = model(users, items, contexts)          # d·ª± ƒëo√°n\n",
    "        loss = loss_fn(preds, labels)                  # t√≠nh l·ªói\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()                                # lan truy·ªÅn ng∆∞·ª£c\n",
    "        optimizer.step()                               # c·∫≠p nh·∫≠t tr·ªçng s·ªë\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "#  ƒê√ÅNH GI√Å HI·ªÜU NƒÇNG M√î H√åNH\n",
    "# -----------------------------------------------------\n",
    "print(\"\\n=== Evaluating ENCM ===\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# ƒê√ÅNH GI√Å HI·ªÜU NƒÇNG M√î H√åNH (CH·ªà T·∫†I K=10)\n",
    "# -----------------------------------------------------\n",
    "print(\"\\n=== Evaluating ENCM ===\")\n",
    "\n",
    "def evaluate_ranking_metrics(model, test_loader, device, K=10):\n",
    "    \"\"\"\n",
    "    ƒê√°nh gi√° m√¥ h√¨nh theo:\n",
    "    - Basic metrics: AUC, RMSE, MAE\n",
    "    - Ranking metrics: Precision@10, Recall@10, Hit@10, MAP@10\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    user_preds = {}\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for users, items, contexts, labels in test_loader:\n",
    "            users, items, contexts = users.to(device), items.to(device), contexts.to(device)\n",
    "            preds = model(users, items, contexts).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "            for u, i, p, l in zip(users.cpu().numpy(), items.cpu().numpy(), preds, labels.numpy()):\n",
    "                if u not in user_preds:\n",
    "                    user_preds[u] = []\n",
    "                user_preds[u].append((i, p, l))\n",
    "\n",
    "    # === Basic metrics ===\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "    rmse = np.sqrt(np.mean((all_labels - all_preds) ** 2))\n",
    "    mae = np.mean(np.abs(all_labels - all_preds))\n",
    "\n",
    "    print(f\"\\n--- Basic Metrics ---\")\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "\n",
    "    # === Ranking metrics @K ===\n",
    "    precisions, recalls, hits, maps = [], [], [], []\n",
    "\n",
    "    for user, predictions in user_preds.items():\n",
    "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "        total_relevant = sum([l for _, _, l in predictions])\n",
    "        if total_relevant == 0:\n",
    "            continue\n",
    "\n",
    "        top_k = predictions[:K]\n",
    "        top_k_labels = [l for _, _, l in top_k]\n",
    "\n",
    "        # Precision, Recall, Hit\n",
    "        precision = sum(top_k_labels) / K\n",
    "        recall = sum(top_k_labels) / total_relevant\n",
    "        hit = 1.0 if sum(top_k_labels) > 0 else 0.0\n",
    "\n",
    "        # MAP@K\n",
    "        ap_sum, correct = 0.0, 0\n",
    "        for idx, (_, _, label) in enumerate(top_k, start=1):\n",
    "            if label == 1:\n",
    "                correct += 1\n",
    "                ap_sum += correct / idx\n",
    "        map_k = ap_sum / min(total_relevant, K)\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        hits.append(hit)\n",
    "        maps.append(map_k)\n",
    "\n",
    "    # In k·∫øt qu·∫£ trung b√¨nh\n",
    "    print(f\"\\n--- Ranking Metrics @ {K} ---\")\n",
    "    print(f\"Precision@{K}: {np.mean(precisions):.4f}\")\n",
    "    print(f\"Recall@{K}:    {np.mean(recalls):.4f}\")\n",
    "    print(f\"Hit@{K}:       {np.mean(hits):.4f}\")\n",
    "    print(f\"MAP@{K}:       {np.mean(maps):.4f}\")\n",
    "\n",
    "# G·ªçi h√†m ƒë√°nh gi√° ch√≠nh x√°c\n",
    "evaluate_ranking_metrics(model, test_loader, device)\n",
    "\n",
    "print(\"\\n‚úÖ ENCM Training & Evaluation Complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fc9d1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ ratings: (4012, 4), context_matrix: (139, 29)\n",
      "üéØ Precision@10: 0.2776\n",
      "üéØ MAP@10: 0.3981\n",
      "‚úÖ K·∫øt qu·∫£ post-filtering ƒë√£ l∆∞u: ./output_carskit_clean\\context_similarity_postfiltered.csv\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# üéØ Context Similarity Post-filtering\n",
    "# =====================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n file\n",
    "base_dir = './output_carskit_clean'\n",
    "ratings_path = os.path.join(base_dir, 'ratings_with_context_id.csv')\n",
    "context_matrix_path = os.path.join(base_dir, 'item_context_matrix.csv')\n",
    "\n",
    "# 1Ô∏è‚É£ ƒê·ªçc d·ªØ li·ªáu\n",
    "ratings_df = pd.read_csv(ratings_path)\n",
    "context_matrix = pd.read_csv(context_matrix_path)\n",
    "print(f\"üìÑ ratings: {ratings_df.shape}, context_matrix: {context_matrix.shape}\")\n",
    "\n",
    "# 2Ô∏è‚É£ X√°c ƒë·ªãnh context hi·ªán t·∫°i (v√≠ d·ª• context_id = 4)\n",
    "CURRENT_CONTEXT = 4\n",
    "\n",
    "# 3Ô∏è‚É£ Chu·∫©n b·ªã vector context hi·ªán t·∫°i\n",
    "context_cols = [c for c in context_matrix.columns if c not in ['item', 'genre']]\n",
    "current_context_vector = np.zeros(len(context_cols))\n",
    "if str(CURRENT_CONTEXT) in context_cols:\n",
    "    idx = context_cols.index(str(CURRENT_CONTEXT))\n",
    "    current_context_vector[idx] = 1\n",
    "else:\n",
    "    raise ValueError(f\"Context_id {CURRENT_CONTEXT} kh√¥ng c√≥ trong context_matrix\")\n",
    "\n",
    "# 4Ô∏è‚É£ T√≠nh cosine similarity gi·ªØa context hi·ªán t·∫°i v√† t·ª´ng item\n",
    "item_vectors = context_matrix[context_cols].values\n",
    "similarity_scores = cosine_similarity(item_vectors, current_context_vector.reshape(1, -1)).flatten()\n",
    "\n",
    "# 5Ô∏è‚É£ G·ªôp similarity v√†o ratings_df\n",
    "sim_df = pd.DataFrame({'item': context_matrix['item'], 'context_similarity': similarity_scores})\n",
    "ratings_df = ratings_df.merge(sim_df, on='item', how='left')\n",
    "\n",
    "# 6Ô∏è‚É£ T√°i x·∫øp h·∫°ng\n",
    "# N·∫øu mu·ªën k·∫øt h·ª£p ƒëi·ªÉm m√¥ h√¨nh g·ªëc (rating) v·ªõi context_similarity:\n",
    "ALPHA = 0.0  # 0.0 = ch·ªâ d√πng context similarity\n",
    "ratings_df['final_score'] = ALPHA * ratings_df['label'] + (1 - ALPHA) * ratings_df['context_similarity']\n",
    "\n",
    "# 7Ô∏è‚É£ H√†m ƒë√°nh gi√° Precision@10 v√† MAP@10\n",
    "def precision_at_k(y_true, y_score, k=10):\n",
    "    top_k = np.argsort(y_score)[-k:][::-1]\n",
    "    return np.mean(np.array(y_true)[top_k])\n",
    "\n",
    "def map_at_k(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.array(y_true)[order][:k]\n",
    "    cum_precisions = [np.mean(y_true[:i+1]) for i in range(len(y_true)) if y_true[i]]\n",
    "    return np.mean(cum_precisions) if cum_precisions else 0.0\n",
    "\n",
    "precisions, maps = [], []\n",
    "for uid, group in ratings_df.groupby('user'):\n",
    "    if group['label'].sum() == 0:\n",
    "        continue\n",
    "    precisions.append(precision_at_k(group['label'], group['final_score'], k=10))\n",
    "    maps.append(map_at_k(group['label'], group['final_score'], k=10))\n",
    "\n",
    "print(f\"üéØ Precision@10: {np.mean(precisions):.4f}\")\n",
    "print(f\"üéØ MAP@10: {np.mean(maps):.4f}\")\n",
    "\n",
    "# 8Ô∏è‚É£ Xu·∫•t file k·∫øt qu·∫£\n",
    "output_path = os.path.join(base_dir, 'context_similarity_postfiltered.csv')\n",
    "ratings_df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "print(f\"‚úÖ K·∫øt qu·∫£ post-filtering ƒë√£ l∆∞u: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
