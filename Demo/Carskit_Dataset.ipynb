{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset về đề xuất nhạc trong ô tô có tính đến ngữ cảnh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import các thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import openpyxl\n",
    "\n",
    "# Thiết lập hiển thị\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Đọc dữ liệu từ file Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kích thước dữ liệu: 4012 hàng x 11 cột\n",
      "Các cột: ['user', 'item', 'label', 'DrivingStyle', 'landscape', 'mood', 'naturalphenomena ', 'RoadType', 'sleepiness', 'trafficConditions', 'weather']\n"
     ]
    }
   ],
   "source": [
    "# Đọc file Excel từ nhiều sheet như trong exploration notebook\n",
    "con_rat = pd.read_excel('./Data_InCarMusic.xlsx', sheet_name=0).rename(columns={' Rating': 'label', 'UserID': 'user', 'ItemID': 'item'})\n",
    "con_fac = pd.read_excel('./Data_InCarMusic.xlsx', sheet_name=1)\n",
    "mus_trk = pd.read_excel('./Data_InCarMusic.xlsx', sheet_name=2).rename(columns={' category_id': 'category_id'})\n",
    "mus_cat = pd.read_excel('./Data_InCarMusic.xlsx', sheet_name=3, header=None).rename(columns={0:'genre_id', 1:'genre'})\n",
    "\n",
    "# Tạo dictionary cho genre mapping\n",
    "mus_cat_dict = mus_cat.set_index('genre_id').genre.str.split(' ').str[0].to_dict()\n",
    "mus_trk['genre'] = mus_trk.category_id.apply(lambda x: mus_cat_dict.get(x))\n",
    "\n",
    "# Sử dụng con_rat làm dataframe chính\n",
    "df = con_rat.copy()\n",
    "\n",
    "print(f'\\nKích thước dữ liệu: {df.shape[0]} hàng x {df.shape[1]} cột')\n",
    "print(f'Các cột: {list(df.columns)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các cột context: ['DrivingStyle', 'landscape', 'mood', 'naturalphenomena ', 'RoadType', 'sleepiness', 'trafficConditions', 'weather']\n",
      "\n",
      "Các giá trị unique cho mỗi cột context:\n",
      "[nan 'relaxed driving' 'sport driving']                 #3\n",
      "[nan 'urban' 'mountains' 'country side' 'coast line']   #5\n",
      "[nan 'sad' 'lazy' 'active' 'happy']                     #5\n",
      "[nan 'night' 'morning' 'day time' 'afternoon']          #5\n",
      "[nan 'city' 'serpentine' 'highway']                     #4\n",
      "[nan 'sleepy' 'awake']                                  #3\n",
      "[nan 'traffic jam' 'lots of cars' 'free road']          #4\n",
      "['sunny' 'snowing' 'rainy' 'cloudy' nan]                #5\n",
      "\n",
      "Sau khi xử lý missing values:\n",
      "Số dòng: 4012\n",
      "Số user: 42\n",
      "Số item: 139\n"
     ]
    }
   ],
   "source": [
    "# Tìm các cột context (categorical)\n",
    "cat_cols = df.columns[df.dtypes == 'object']\n",
    "print(\"Các cột context:\", list(cat_cols))\n",
    "\n",
    "# In ra các giá trị unique cho mỗi cột context\n",
    "print(\"\\nCác giá trị unique cho mỗi cột context:\")\n",
    "for c in cat_cols:\n",
    "    unique_vals = df[c].unique()\n",
    "    print(f'{str(unique_vals):<55} #{len(unique_vals)}')\n",
    "\n",
    "# Tạo binary label như trong exploration\n",
    "df['label'] = np.select([df.label > 3, df.label <= 3], [1, 0])\n",
    "\n",
    "# Xử lý missing values - thay thế NaN bằng 'Unknown' thay vì loại bỏ\n",
    "for col in cat_cols:\n",
    "    df[col] = df[col].fillna('Unknown')\n",
    "\n",
    "print(f\"\\nSau khi xử lý missing values:\")\n",
    "print(f\"Số dòng: {len(df)}\")\n",
    "print(f\"Số user: {df['user'].nunique()}\")\n",
    "print(f\"Số item: {df['item'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe sau khi xử lý context:\n",
      "   user  item  label                                            context\n",
      "0  1001   715      0  DrivingStyle_Unknownlandscape_Unknownmood_Unkn...\n",
      "1  1001   267      1  DrivingStyle_Unknownlandscape_Unknownmood_Unkn...\n",
      "2  1001   294      0  DrivingStyle_Unknownlandscape_Unknownmood_Unkn...\n",
      "3  1001   259      1  DrivingStyle_Unknownlandscape_Unknownmood_Unkn...\n",
      "4  1001   674      0  DrivingStyle_Unknownlandscape_Unknownmood_Unkn...\n",
      "\n",
      "Số lượng context unique: 27\n",
      "Các context mẫu: ['DrivingStyle_Unknownlandscape_Unknownmood_Unknownnaturalphenomena _UnknownRoadType_Unknownsleepiness_UnknowntrafficConditions_Unknownweather_sunny'\n",
      " 'DrivingStyle_Unknownlandscape_Unknownmood_Unknownnaturalphenomena _UnknownRoadType_Unknownsleepiness_UnknowntrafficConditions_Unknownweather_snowing'\n",
      " 'DrivingStyle_Unknownlandscape_Unknownmood_Unknownnaturalphenomena _UnknownRoadType_Unknownsleepiness_UnknowntrafficConditions_Unknownweather_rainy'\n",
      " 'DrivingStyle_Unknownlandscape_Unknownmood_Unknownnaturalphenomena _UnknownRoadType_Unknownsleepiness_UnknowntrafficConditions_Unknownweather_cloudy'\n",
      " 'DrivingStyle_Unknownlandscape_Unknownmood_Unknownnaturalphenomena _UnknownRoadType_Unknownsleepiness_UnknowntrafficConditions_traffic jamweather_Unknown'\n",
      " 'DrivingStyle_Unknownlandscape_Unknownmood_Unknownnaturalphenomena _UnknownRoadType_Unknownsleepiness_UnknowntrafficConditions_lots of carsweather_Unknown'\n",
      " 'DrivingStyle_Unknownlandscape_Unknownmood_Unknownnaturalphenomena _UnknownRoadType_Unknownsleepiness_sleepytrafficConditions_Unknownweather_Unknown'\n",
      " 'DrivingStyle_Unknownlandscape_Unknownmood_Unknownnaturalphenomena _UnknownRoadType_Unknownsleepiness_awaketrafficConditions_Unknownweather_Unknown'\n",
      " 'DrivingStyle_Unknownlandscape_Unknownmood_Unknownnaturalphenomena _UnknownRoadType_citysleepiness_UnknowntrafficConditions_Unknownweather_Unknown'\n",
      " 'DrivingStyle_Unknownlandscape_Unknownmood_Unknownnaturalphenomena _nightRoadType_Unknownsleepiness_UnknowntrafficConditions_Unknownweather_Unknown']\n"
     ]
    }
   ],
   "source": [
    "# Xử lý context với one-hot encoding như trong exploration notebook\n",
    "# Tạo one-hot encoding cho các cột context\n",
    "features_oh = pd.get_dummies(df[cat_cols], dummy_na=True)\n",
    "\n",
    "# Tạo context string từ one-hot encoding\n",
    "context_series = (features_oh * features_oh.columns).sum(axis=1)\n",
    "context_series.rename('context', inplace=True)\n",
    "\n",
    "# Tạo dataframe cuối cùng với context\n",
    "feat_data = pd.concat([df[['user', 'item', 'label']], context_series], axis=1)\n",
    "\n",
    "print(\"Dataframe sau khi xử lý context:\")\n",
    "print(feat_data.head())\n",
    "print(f\"\\nSố lượng context unique: {feat_data['context'].nunique()}\")\n",
    "print(f\"Các context mẫu: {feat_data['context'].unique()[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu các file:\n",
      " - output_carskit\\ratings_with_context.csv\n",
      " - output_carskit\\ratings_only.csv\n",
      " - output_carskit\\item_features.csv\n",
      "\n",
      "Thống kê cuối cùng:\n",
      "Số user: 42\n",
      "Số item: 139\n",
      "Số interaction: 4012\n",
      "Số context unique: 27\n",
      "Tỷ lệ positive (label=1): 0.258\n"
     ]
    }
   ],
   "source": [
    "# Xuất dữ liệu \n",
    "import os\n",
    "out_dir = 'output_carskit'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Xuất ratings với context (cho LightFM)\n",
    "ratings_ctx_path = os.path.join(out_dir, 'ratings_with_context.csv')\n",
    "feat_data.to_csv(ratings_ctx_path, index=False, encoding='utf-8')\n",
    "\n",
    "# Xuất ratings chỉ có user, item, label (cho collaborative filtering)\n",
    "ratings_only = feat_data[['user', 'item', 'label']].copy()\n",
    "ratings_only_path = os.path.join(out_dir, 'ratings_only.csv')\n",
    "ratings_only.to_csv(ratings_only_path, index=False, encoding='utf-8')\n",
    "\n",
    "# Xuất thông tin item với genre (cho item features)\n",
    "item_features = mus_trk[['id', 'genre']].rename(columns={'id': 'item'})\n",
    "item_features_path = os.path.join(out_dir, 'item_features.csv')\n",
    "item_features.to_csv(item_features_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Đã lưu các file:\")\n",
    "print(f\" - {ratings_ctx_path}\")\n",
    "print(f\" - {ratings_only_path}\")\n",
    "print(f\" - {item_features_path}\")\n",
    "\n",
    "# Thống kê cuối cùng\n",
    "print(f\"\\nThống kê cuối cùng:\")\n",
    "print(f\"Số user: {feat_data['user'].nunique()}\")\n",
    "print(f\"Số item: {feat_data['item'].nunique()}\")\n",
    "print(f\"Số interaction: {len(feat_data)}\")\n",
    "print(f\"Số context unique: {feat_data['context'].nunique()}\")\n",
    "print(f\"Tỷ lệ positive (label=1): {feat_data['label'].mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train mô hình gợi ý"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user item  label\n",
      "0  1001  715      0\n",
      "1  1001  267      1\n",
      "2  1001  294      0\n",
      "3  1001  259      1\n",
      "4  1001  674      0\n",
      "   user item  label                                            context\n",
      "0  1001  715      0  DrivingStyle_Unknownlandscape_Unknownmood_Unkn...\n",
      "1  1001  267      1  DrivingStyle_Unknownlandscape_Unknownmood_Unkn...\n",
      "2  1001  294      0  DrivingStyle_Unknownlandscape_Unknownmood_Unkn...\n",
      "3  1001  259      1  DrivingStyle_Unknownlandscape_Unknownmood_Unkn...\n",
      "4  1001  674      0  DrivingStyle_Unknownlandscape_Unknownmood_Unkn...\n",
      "Users: 42 Items: 139 Interactions: 4012\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from models.model_classes import BMF, NeuMF, LNCM, ENCM\n",
    "\n",
    "DATA_DIR = r\"D:\\DATN-CNTT-2025-CoLien-B21DCCN433\\Demo\\output_carskit\"\n",
    "RATINGS_ONLY = os.path.join(DATA_DIR, \"ratings_only.csv\")\n",
    "RATINGS_CTX = os.path.join(DATA_DIR, \"ratings_with_context.csv\")\n",
    "\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "df_only = pd.read_csv(RATINGS_ONLY)          # columns: user, item, label\n",
    "df_ctx = pd.read_csv(RATINGS_CTX)            # columns: user, item, label, context (string)\n",
    "\n",
    "# Binary labels already prepared in your pipeline (0/1)\n",
    "df_only = df_only.astype({\"user\": str, \"item\": str, \"label\": int})\n",
    "df_ctx = df_ctx.astype({\"user\": str, \"item\": str, \"label\": int, \"context\": str})\n",
    "\n",
    "print(df_only.head())\n",
    "print(df_ctx.head())\n",
    "print(\"Users:\", df_only[\"user\"].nunique(), \"Items:\", df_only[\"item\"].nunique(), \"Interactions:\", len(df_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2863, 341, 808, 2863, 341, 808)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_le = LabelEncoder().fit(df_only[\"user\"].values)\n",
    "item_le = LabelEncoder().fit(df_only[\"item\"].values)\n",
    "\n",
    "# Use the combined 'context' string as one categorical feature for ENCM\n",
    "ctx_le = LabelEncoder().fit(df_ctx[\"context\"].values)\n",
    "\n",
    "df_only[\"u_idx\"] = user_le.transform(df_only[\"user\"])\n",
    "df_only[\"i_idx\"] = item_le.transform(df_only[\"item\"])\n",
    "\n",
    "df_ctx[\"u_idx\"] = user_le.transform(df_ctx[\"user\"])\n",
    "df_ctx[\"i_idx\"] = item_le.transform(df_ctx[\"item\"])\n",
    "df_ctx[\"c_idx\"] = ctx_le.transform(df_ctx[\"context\"])\n",
    "\n",
    "n_users = df_only[\"u_idx\"].max() + 1\n",
    "n_items = df_only[\"i_idx\"].max() + 1\n",
    "n_contexts = [df_ctx[\"c_idx\"].max() + 1]  # single context field\n",
    "\n",
    "n_users, n_items, n_contexts\n",
    "\n",
    "def per_user_split(df, user_col=\"u_idx\", frac_test=0.2, frac_val=0.1, seed=SEED):\n",
    "    train_rows, test_rows = [], []\n",
    "    for u, grp in df.groupby(user_col):\n",
    "        if len(grp) < 2:\n",
    "            train_rows.append(grp)\n",
    "            continue\n",
    "        tr, te = train_test_split(grp, test_size=frac_test, random_state=seed, shuffle=True)\n",
    "        train_rows.append(tr)\n",
    "        test_rows.append(te)\n",
    "    train_df = pd.concat(train_rows).reset_index(drop=True)\n",
    "    test_df = pd.concat(test_rows).reset_index(drop=True) if test_rows else train_df.iloc[0:0].copy()\n",
    "\n",
    "    # val from train\n",
    "    if len(train_df) > 1:\n",
    "        tr_rows, val_rows = [], []\n",
    "        for u, grp in train_df.groupby(user_col):\n",
    "            if len(grp) < 2:\n",
    "                tr_rows.append(grp)\n",
    "                continue\n",
    "            tr, va = train_test_split(grp, test_size=frac_val, random_state=seed, shuffle=True)\n",
    "            tr_rows.append(tr)\n",
    "            val_rows.append(va)\n",
    "        train_df = pd.concat(tr_rows).reset_index(drop=True)\n",
    "        val_df = pd.concat(val_rows).reset_index(drop=True) if val_rows else train_df.iloc[0:0].copy()\n",
    "    else:\n",
    "        val_df = train_df.iloc[0:0].copy()\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_only, val_only, test_only = per_user_split(df_only)\n",
    "train_ctx,  val_ctx,  test_ctx  = per_user_split(df_ctx)\n",
    "\n",
    "len(train_only), len(val_only), len(test_only), len(train_ctx), len(val_ctx), len(test_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_positives(df, user_col=\"u_idx\", item_col=\"i_idx\", label_col=\"label\"):\n",
    "    pos = defaultdict(set)\n",
    "    for u, it, y in df[[user_col, item_col, label_col]].itertuples(index=False):\n",
    "        if y == 1:\n",
    "            pos[u].add(it)\n",
    "    return pos\n",
    "\n",
    "train_pos = build_user_positives(train_only)\n",
    "all_items = np.arange(n_items, dtype=np.int32)\n",
    "\n",
    "# Increase negatives for harder training\n",
    "DEF_NUM_NEG = 10\n",
    "\n",
    "def make_pairs(df, user_col=\"u_idx\", item_col=\"i_idx\", label_col=\"label\", num_neg=DEF_NUM_NEG):\n",
    "    users, items, labels = [], [], []\n",
    "    by_user = df.groupby(user_col)\n",
    "    for u, grp in by_user:\n",
    "        pos_items = set(grp.loc[grp[label_col] == 1, item_col].tolist())\n",
    "        if not pos_items:\n",
    "            continue\n",
    "        for it in pos_items:\n",
    "            users.append(u); items.append(it); labels.append(1)\n",
    "            # Negatives\n",
    "            neg_count = 0\n",
    "            while neg_count < num_neg:\n",
    "                j = np.random.randint(0, n_items)\n",
    "                if (j not in pos_items):\n",
    "                    users.append(u); items.append(j); labels.append(0)\n",
    "                    neg_count += 1\n",
    "    return np.array(users, dtype=np.int32), np.array(items, dtype=np.int32), np.array(labels, dtype=np.float32)\n",
    "\n",
    "tr_u, tr_i, tr_y = make_pairs(train_only)\n",
    "va_u, va_i, va_y = make_pairs(val_only) if len(val_only) else (tr_u[:0], tr_i[:0], tr_y[:0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo tf.data.Dataset cho mô hình 2-input (BMF/NeuMF/LNCM) và 3-input (ENCM)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "SHUFFLE_BUF = 100_000\n",
    "\n",
    "# 2-input datasets: (user_idx, item_idx) -> label\n",
    "train_ds_2 = tf.data.Dataset.from_tensor_slices(((tr_u, tr_i), tr_y)) \\\n",
    "    .shuffle(min(len(tr_y), SHUFFLE_BUF), seed=SEED, reshuffle_each_iteration=True) \\\n",
    "    .batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds_2 = tf.data.Dataset.from_tensor_slices(((va_u, va_i), va_y)) \\\n",
    "    .batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# 3-input datasets cho ENCM: (user_idx, item_idx, context_idx[None]) -> label\n",
    "# Tạo cặp có ngữ cảnh từ tập đã có c_idx (train_ctx/val_ctx)\n",
    "from collections import defaultdict\n",
    "\n",
    "def make_pairs_with_ctx(df, user_col=\"u_idx\", item_col=\"i_idx\", ctx_col=\"c_idx\", label_col=\"label\", num_neg=DEF_NUM_NEG):\n",
    "    users, items, ctxs, labels = [], [], [], []\n",
    "    by_user = df.groupby(user_col)\n",
    "    for u, grp in by_user:\n",
    "        pos_rows = grp.loc[grp[label_col] == 1, [item_col, ctx_col]]\n",
    "        pos_items = set(pos_rows[item_col].tolist())\n",
    "        if not len(pos_rows):\n",
    "            continue\n",
    "        for it, c in pos_rows.itertuples(index=False):\n",
    "            # positive\n",
    "            users.append(u); items.append(it); ctxs.append(c); labels.append(1.0)\n",
    "            # negatives với cùng context của positive để giữ ngữ cảnh ổn định\n",
    "            neg_count = 0\n",
    "            while neg_count < num_neg:\n",
    "                j = np.random.randint(0, n_items)\n",
    "                if j not in pos_items:\n",
    "                    users.append(u); items.append(j); ctxs.append(c); labels.append(0.0)\n",
    "                    neg_count += 1\n",
    "    return (\n",
    "        np.array(users, dtype=np.int32),\n",
    "        np.array(items, dtype=np.int32),\n",
    "        np.array(ctxs, dtype=np.int32),\n",
    "        np.array(labels, dtype=np.float32),\n",
    "    )\n",
    "\n",
    "tr_u3, tr_i3, tr_c3, tr_y3 = make_pairs_with_ctx(train_ctx)\n",
    "va_u3, va_i3, va_c3, va_y3 = make_pairs_with_ctx(val_ctx) if len(val_ctx) else (tr_u3[:0], tr_i3[:0], tr_c3[:0], tr_y3[:0])\n",
    "\n",
    "# reshape context sang (N, 1) như ENCM.build([(None,), (None,), (None, 1)])\n",
    "tr_c3_inp = tr_c3.reshape(-1, 1)\n",
    "va_c3_inp = va_c3.reshape(-1, 1)\n",
    "\n",
    "train_ds_3 = tf.data.Dataset.from_tensor_slices(((tr_u3, tr_i3, tr_c3_inp), tr_y3)) \\\n",
    "    .shuffle(min(len(tr_y3), SHUFFLE_BUF), seed=SEED, reshuffle_each_iteration=True) \\\n",
    "    .batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds_3 = tf.data.Dataset.from_tensor_slices(((va_u3, va_i3, va_c3_inp), va_y3)) \\\n",
    "    .batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Lưu train_encm để sử dụng ở phần builder đánh giá top-K (lấy mode context per-user)\n",
    "train_encm = train_ctx[[\"u_idx\", \"i_idx\", \"c_idx\", \"label\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_binary(model, lr=1e-3):\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[tf.keras.metrics.AUC(name=\"auc\"), tf.keras.metrics.BinaryAccuracy(name=\"acc\")]\n",
    "    )\n",
    "\n",
    "def fit_model(model, train_ds, val_ds, epochs=5):\n",
    "    cb = [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=2, restore_best_weights=True)\n",
    "    ]\n",
    "    hist = model.fit(train_ds, validation_data=val_ds, epochs=epochs, verbose=1, callbacks=cb)\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bmf_17', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.7605 - auc: 0.5109 - loss: 0.6744 - val_acc: 0.8792 - val_auc: 0.5246 - val_loss: 0.6530\n",
      "Epoch 2/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.9002 - auc: 0.5715 - loss: 0.6387 - val_acc: 0.9091 - val_auc: 0.5393 - val_loss: 0.6199\n",
      "Epoch 3/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.9091 - auc: 0.5959 - loss: 0.6029 - val_acc: 0.9091 - val_auc: 0.5538 - val_loss: 0.5850\n",
      "Epoch 4/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.9091 - auc: 0.6114 - loss: 0.5625 - val_acc: 0.9091 - val_auc: 0.5642 - val_loss: 0.5448\n",
      "Epoch 5/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.9091 - auc: 0.6436 - loss: 0.5140 - val_acc: 0.9091 - val_auc: 0.5760 - val_loss: 0.4996\n",
      "Epoch 6/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.9091 - auc: 0.6624 - loss: 0.4608 - val_acc: 0.9091 - val_auc: 0.5861 - val_loss: 0.4527\n",
      "Epoch 7/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.9091 - auc: 0.6826 - loss: 0.4091 - val_acc: 0.9091 - val_auc: 0.5932 - val_loss: 0.4123\n",
      "Epoch 8/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.9091 - auc: 0.7093 - loss: 0.3659 - val_acc: 0.9091 - val_auc: 0.6003 - val_loss: 0.3827\n",
      "Epoch 9/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.9091 - auc: 0.7285 - loss: 0.3335 - val_acc: 0.9091 - val_auc: 0.6088 - val_loss: 0.3615\n",
      "Epoch 10/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.9091 - auc: 0.7497 - loss: 0.3091 - val_acc: 0.9078 - val_auc: 0.6184 - val_loss: 0.3463\n",
      "Epoch 11/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.9093 - auc: 0.7727 - loss: 0.2907 - val_acc: 0.9078 - val_auc: 0.6258 - val_loss: 0.3366\n",
      "Epoch 12/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.9093 - auc: 0.7939 - loss: 0.2765 - val_acc: 0.9078 - val_auc: 0.6366 - val_loss: 0.3294\n",
      "Epoch 13/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.9096 - auc: 0.8150 - loss: 0.2647 - val_acc: 0.9065 - val_auc: 0.6470 - val_loss: 0.3239\n",
      "Epoch 14/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.9101 - auc: 0.8349 - loss: 0.2544 - val_acc: 0.9065 - val_auc: 0.6567 - val_loss: 0.3205\n",
      "Epoch 15/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.9106 - auc: 0.8526 - loss: 0.2453 - val_acc: 0.8987 - val_auc: 0.6651 - val_loss: 0.3186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x188972f2260>"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bmf = BMF(n_users=n_users, n_items=n_items, embedding_dim=50)\n",
    "# build shapes\n",
    "bmf.build([(None,), (None,)])\n",
    "compile_binary(bmf, lr=1e-3)\n",
    "fit_model(bmf, train_ds_2, val_ds_2, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'neu_mf_14', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - acc: 0.8588 - auc: 0.4805 - loss: 0.6745 - val_acc: 0.9091 - val_auc: 0.5135 - val_loss: 0.6380\n",
      "Epoch 2/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.9091 - auc: 0.5192 - loss: 0.5774 - val_acc: 0.9091 - val_auc: 0.5140 - val_loss: 0.4709\n",
      "Epoch 3/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.9091 - auc: 0.5136 - loss: 0.3878 - val_acc: 0.9091 - val_auc: 0.5253 - val_loss: 0.3188\n",
      "Epoch 4/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.9091 - auc: 0.5222 - loss: 0.3354 - val_acc: 0.9091 - val_auc: 0.5524 - val_loss: 0.3109\n",
      "Epoch 5/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.9091 - auc: 0.5948 - loss: 0.3097 - val_acc: 0.9091 - val_auc: 0.5828 - val_loss: 0.3044\n",
      "Epoch 6/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.9091 - auc: 0.6609 - loss: 0.2950 - val_acc: 0.9091 - val_auc: 0.5979 - val_loss: 0.3029\n",
      "Epoch 7/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.9091 - auc: 0.6652 - loss: 0.2933 - val_acc: 0.9091 - val_auc: 0.6006 - val_loss: 0.3046\n",
      "Epoch 8/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.9091 - auc: 0.6960 - loss: 0.2839 - val_acc: 0.9091 - val_auc: 0.6133 - val_loss: 0.3048\n",
      "Epoch 9/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.9091 - auc: 0.6974 - loss: 0.2860 - val_acc: 0.9091 - val_auc: 0.6181 - val_loss: 0.3074\n",
      "Epoch 10/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.9091 - auc: 0.7052 - loss: 0.2832 - val_acc: 0.9091 - val_auc: 0.6268 - val_loss: 0.3083\n",
      "Epoch 11/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.9091 - auc: 0.7281 - loss: 0.2762 - val_acc: 0.9091 - val_auc: 0.6318 - val_loss: 0.3086\n",
      "Epoch 12/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9091 - auc: 0.7519 - loss: 0.2674 - val_acc: 0.9091 - val_auc: 0.6390 - val_loss: 0.3071\n",
      "Epoch 13/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9104 - auc: 0.7706 - loss: 0.2610 - val_acc: 0.9052 - val_auc: 0.6502 - val_loss: 0.3083\n",
      "Epoch 14/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9124 - auc: 0.7968 - loss: 0.2507 - val_acc: 0.9013 - val_auc: 0.6632 - val_loss: 0.3085\n",
      "Epoch 15/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9144 - auc: 0.8191 - loss: 0.2408 - val_acc: 0.8896 - val_auc: 0.6738 - val_loss: 0.3134\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x188afa47310>"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neumf = NeuMF(n_users=n_users, n_items=n_items, embedding_dim=50, hidden_dims=[64,32,16])\n",
    "neumf.build([(None,), (None,)])\n",
    "compile_binary(neumf, lr=1e-3)\n",
    "fit_model(neumf, train_ds_2, val_ds_2, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'lncm_14', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - acc: 0.9091 - auc: 0.5098 - loss: 0.3376 - val_acc: 0.9091 - val_auc: 0.5421 - val_loss: 0.3083\n",
      "Epoch 2/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.9091 - auc: 0.6343 - loss: 0.2940 - val_acc: 0.9091 - val_auc: 0.5901 - val_loss: 0.2988\n",
      "Epoch 3/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.9091 - auc: 0.7008 - loss: 0.2815 - val_acc: 0.9091 - val_auc: 0.5981 - val_loss: 0.3153\n",
      "Epoch 4/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.9091 - auc: 0.7171 - loss: 0.2780 - val_acc: 0.9091 - val_auc: 0.6054 - val_loss: 0.3151\n",
      "Epoch 5/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.9091 - auc: 0.7205 - loss: 0.2762 - val_acc: 0.9091 - val_auc: 0.6073 - val_loss: 0.3154\n",
      "Epoch 6/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.9091 - auc: 0.7225 - loss: 0.2760 - val_acc: 0.9091 - val_auc: 0.6145 - val_loss: 0.3149\n",
      "Epoch 7/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.9091 - auc: 0.7283 - loss: 0.2744 - val_acc: 0.9091 - val_auc: 0.6153 - val_loss: 0.3153\n",
      "Epoch 8/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.9091 - auc: 0.7300 - loss: 0.2732 - val_acc: 0.9091 - val_auc: 0.6151 - val_loss: 0.3173\n",
      "Epoch 9/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.9091 - auc: 0.7307 - loss: 0.2723 - val_acc: 0.9091 - val_auc: 0.6152 - val_loss: 0.3170\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x188b12056c0>"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lncm = LNCM(n_users=n_users, n_items=n_items, embedding_dim=50, hidden_dims=[64,32])\n",
    "lncm.build([(None,), (None,)])\n",
    "compile_binary(lncm, lr=1e-3)\n",
    "fit_model(lncm, train_ds_2, val_ds_2, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'encm_14', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8876 - auc: 0.5128 - loss: 0.5355 - val_acc: 0.9091 - val_auc: 0.5400 - val_loss: 0.3286\n",
      "Epoch 2/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.9091 - auc: 0.5829 - loss: 0.3105 - val_acc: 0.9091 - val_auc: 0.6272 - val_loss: 0.2978\n",
      "Epoch 3/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.9091 - auc: 0.7063 - loss: 0.2829 - val_acc: 0.9091 - val_auc: 0.6385 - val_loss: 0.3004\n",
      "Epoch 4/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.9091 - auc: 0.7387 - loss: 0.2732 - val_acc: 0.9091 - val_auc: 0.6385 - val_loss: 0.3093\n",
      "Epoch 5/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.9091 - auc: 0.7431 - loss: 0.2715 - val_acc: 0.9091 - val_auc: 0.6379 - val_loss: 0.3099\n",
      "Epoch 6/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.9091 - auc: 0.7572 - loss: 0.2659 - val_acc: 0.9091 - val_auc: 0.6392 - val_loss: 0.3081\n",
      "Epoch 7/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.9096 - auc: 0.7683 - loss: 0.2620 - val_acc: 0.9103 - val_auc: 0.6555 - val_loss: 0.3049\n",
      "Epoch 8/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.9149 - auc: 0.7727 - loss: 0.2557 - val_acc: 0.9079 - val_auc: 0.6609 - val_loss: 0.2995\n",
      "Epoch 9/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.9213 - auc: 0.7937 - loss: 0.2434 - val_acc: 0.9055 - val_auc: 0.6773 - val_loss: 0.2946\n",
      "Epoch 10/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.9288 - auc: 0.8201 - loss: 0.2272 - val_acc: 0.8958 - val_auc: 0.6919 - val_loss: 0.2998\n",
      "Epoch 11/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.9309 - auc: 0.8503 - loss: 0.2117 - val_acc: 0.8958 - val_auc: 0.7005 - val_loss: 0.3229\n",
      "Epoch 12/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.9318 - auc: 0.8694 - loss: 0.2012 - val_acc: 0.8945 - val_auc: 0.7072 - val_loss: 0.3459\n",
      "Epoch 13/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.9331 - auc: 0.8830 - loss: 0.1922 - val_acc: 0.8909 - val_auc: 0.7131 - val_loss: 0.3841\n",
      "Epoch 14/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.9352 - auc: 0.8922 - loss: 0.1859 - val_acc: 0.8921 - val_auc: 0.7235 - val_loss: 0.3997\n",
      "Epoch 15/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.9368 - auc: 0.8999 - loss: 0.1801 - val_acc: 0.8861 - val_auc: 0.7317 - val_loss: 0.4117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x188b24aaad0>"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encm = ENCM(n_users=n_users, n_items=n_items, n_contexts=n_contexts, embedding_dim=50, context_dim=10, hidden_dims=[64,32])\n",
    "# Inputs: user_ids, item_ids, context_features (shape: (batch, 1))\n",
    "encm.build([(None,), (None,), (None, 1)])\n",
    "compile_binary(encm, lr=1e-3)\n",
    "fit_model(encm, train_ds_3, val_ds_3, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model    HR@10  nDCG@10  Precision@10   MAP@10\n",
      "  BMF 0.502370 0.413544      0.050237 0.386371\n",
      "NeuMF 0.497630 0.377705      0.049763 0.340038\n",
      " LNCM 0.336493 0.177495      0.033649 0.128987\n",
      " ENCM 0.502370 0.421782      0.050237 0.397418\n"
     ]
    }
   ],
   "source": [
    "# Leave-One-Out style evaluation (99 negatives per test positive), with correct context for ENCM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Build dictionaries of user->seen items across splits to avoid leakage\n",
    "user_seen = defaultdict(set)\n",
    "for df_ in [train_only, val_only, test_only]:\n",
    "    for u, it, y in df_[[\"u_idx\",\"i_idx\",\"label\"]].itertuples(index=False):\n",
    "        if y == 1:\n",
    "            user_seen[u].add(it)\n",
    "\n",
    "# Test positives as interaction list\n",
    "test_pos_rows = test_only.loc[test_only[\"label\"] == 1, [\"u_idx\",\"i_idx\"]].copy()\n",
    "# For ENCM, map to context id from test_ctx\n",
    "test_ctx_map = {}\n",
    "if 'test_ctx' in globals() and len(test_ctx):\n",
    "    # build mapping (u,i)->c; if multiple rows exist, use the most frequent\n",
    "    tmp = test_ctx.groupby([\"u_idx\",\"i_idx\"])['c_idx'].agg(lambda s: s.value_counts().index[0])\n",
    "    test_ctx_map = tmp.to_dict()\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "\n",
    "def evaluate_loo(model, builder, k=10, num_neg=99):\n",
    "    hits, ndcgs, precs, maps = [], [], [], []\n",
    "\n",
    "    def dcg_at_k(rank):\n",
    "        return 1.0 / np.log2(rank + 2)  # rank is 0-indexed\n",
    "\n",
    "    for u, i_pos in test_pos_rows[[\"u_idx\",\"i_idx\"]].itertuples(index=False):\n",
    "        seen = user_seen.get(u, set()) - {i_pos}\n",
    "        # sample negatives that are not seen\n",
    "        neg_pool = [j for j in range(n_items) if j not in seen and j != i_pos]\n",
    "        if len(neg_pool) < num_neg:\n",
    "            # fallback: use whatever available\n",
    "            sampled_negs = neg_pool\n",
    "        else:\n",
    "            sampled_negs = rng.choice(neg_pool, size=num_neg, replace=False).tolist()\n",
    "\n",
    "        candidates = np.array([i_pos] + sampled_negs, dtype=np.int32)\n",
    "        batch_user = np.full_like(candidates, u, dtype=np.int32)\n",
    "\n",
    "        # context for ENCM: use exact test context if available, else per-user mode, else 0\n",
    "        c_id = int(test_ctx_map.get((int(u), int(i_pos)),  \n",
    "                 user_ctx_mode.get(int(u), 0)))\n",
    "        model_inputs = builder(batch_user, candidates, u, c_id)\n",
    "\n",
    "        scores = model(model_inputs, training=False).numpy().reshape(-1)\n",
    "        top_idx = np.argsort(-scores)[:k]\n",
    "        top_items = candidates[top_idx].tolist()\n",
    "\n",
    "        # metrics\n",
    "        hit = 1.0 if i_pos in top_items else 0.0\n",
    "        hits.append(hit)\n",
    "\n",
    "        # nDCG@k\n",
    "        if hit:\n",
    "            rank = top_items.index(i_pos)  # 0-indexed\n",
    "            ndcgs.append(dcg_at_k(rank))\n",
    "        else:\n",
    "            ndcgs.append(0.0)\n",
    "\n",
    "        # Precision@k (either 0 or 1/k because only one positive)\n",
    "        precs.append(1.0/ k if hit else 0.0)\n",
    "\n",
    "        # MAP@k equals precision at rank of the hit when it occurs\n",
    "        if hit:\n",
    "            r = top_items.index(i_pos) + 1\n",
    "            maps.append(1.0 / r)\n",
    "        else:\n",
    "            maps.append(0.0)\n",
    "\n",
    "    return {\n",
    "        'HR@{}'.format(k): float(np.mean(hits)) if hits else 0.0,\n",
    "        'nDCG@{}'.format(k): float(np.mean(ndcgs)) if ndcgs else 0.0,\n",
    "        'Precision@{}'.format(k): float(np.mean(precs)) if precs else 0.0,\n",
    "        'MAP@{}'.format(k): float(np.mean(maps)) if maps else 0.0,\n",
    "    }\n",
    "\n",
    "# Builders updated to accept explicit context id for ENCM\n",
    "def two_inp_builder_eval(u_arr, i_arr, u_single, c_id_unused):\n",
    "    return (u_arr, i_arr)\n",
    "\n",
    "def three_inp_builder_eval(u_arr, i_arr, u_single, c_id):\n",
    "    c_arr = np.full((len(u_arr), 1), c_id, dtype=np.int32)\n",
    "    return (u_arr, i_arr, c_arr)\n",
    "\n",
    "models_eval = {\n",
    "    'BMF': bmf,\n",
    "    'NeuMF': neumf,\n",
    "    'LNCM': lncm,\n",
    "    'ENCM': encm,\n",
    "}\n",
    "\n",
    "builders_eval = {\n",
    "    'BMF': two_inp_builder_eval,\n",
    "    'NeuMF': two_inp_builder_eval,\n",
    "    'LNCM': two_inp_builder_eval,\n",
    "    'ENCM': three_inp_builder_eval,\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for name, model in models_eval.items():\n",
    "    metrics = evaluate_loo(model, builders_eval[name], k=10, num_neg=99)\n",
    "    row = {'model': name}\n",
    "    row.update(metrics)\n",
    "    rows.append(row)\n",
    "\n",
    "results_loo = pd.DataFrame(rows)\n",
    "print(results_loo.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
